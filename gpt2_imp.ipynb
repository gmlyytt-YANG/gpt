{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 09:33:04.052993: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "382688it [00:50, 7632.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from numpy import arange\n",
    "import math\n",
    "import pyecharts\n",
    "import sys,base64,urllib,re\n",
    "import warnings \n",
    "from optparse import OptionParser\n",
    "import logging\n",
    "import logging.config\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "# 加载分类数据\n",
    "from tqdm import tqdm\n",
    "corpus = []\n",
    "MAX_LENGTH = 100\n",
    "cat_name_dic = {\n",
    "    '100': '民生',\n",
    "    '101': '文化',\n",
    "    '102': '娱乐',\n",
    "    '103': '体育',\n",
    "    '104': '财经',\n",
    "    '106': '房产',\n",
    "    '107': '汽车',\n",
    "    '108': '教育',\n",
    "    '109': '科技',\n",
    "    '110': '军事',\n",
    "    '112': '旅游',\n",
    "    '113': '国际',\n",
    "    '114': '证券',\n",
    "    '115': '农业',\n",
    "    '116': '电竞'\n",
    "}\n",
    "with open('./toutiao_cat_data.txt', 'r', encoding='utf-8') as f:\n",
    "    for k in tqdm(f):\n",
    "        new_id, cat, cat_n, title, title_kws = k.strip(\"\").split(\"_!_\")\n",
    "        cat_name = cat_name_dic.get(cat, '')\n",
    "        if cat_name == \"\":\n",
    "            continue\n",
    "        title = title.replace(\"|\", \"\")\n",
    "        title = title + \"|\" + cat_name\n",
    "        if len(title) > MAX_LENGTH:\n",
    "            continue\n",
    "        title = \" \".join(jieba.cut(title, cut_all=False))\n",
    "        corpus.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382675/382675 [00:00<00:00, 515393.78it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_format = []\n",
    "for k in tqdm(corpus):\n",
    "    k = k.split(\" \")\n",
    "    corpus_format.append(\" \".join(k))\n",
    "    \n",
    "random.shuffle(corpus_format)\n",
    "train_examples = corpus_format\n",
    "\n",
    "tokenizer_title = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (k for k in train_examples), target_vocab_size=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang):\n",
    "    lang = [tokenizer_title.vocab_size] + tokenizer_title.encode(lang) + [tokenizer_title.vocab_size + 1]\n",
    "    return lang\n",
    "\n",
    "def pad_with_zero(lang, max_length=MAX_LENGTH):\n",
    "    n = MAX_LENGTH - len(lang)\n",
    "    lang = lang + [0 for k in range(n)]\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 09:37:44.437039: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-18 09:37:44.476612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3f:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2023-06-18 09:37:44.476645: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-18 09:37:44.479863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-18 09:37:44.479938: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-18 09:37:44.480837: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-18 09:37:44.481133: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-18 09:37:44.481881: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-06-18 09:37:44.482516: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-06-18 09:37:44.482690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-18 09:37:44.483432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-18 09:37:44.484116: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 09:37:44.488161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3f:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2023-06-18 09:37:44.488805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-18 09:37:44.488842: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-18 09:37:45.038245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-18 09:37:45.038265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-06-18 09:37:45.038271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-06-18 09:37:45.039437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8098 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:3f:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "train_examples = corpus_format[:300000]\n",
    "train_examples = [encode(k) for k in train_examples]\n",
    "train_examples = [k for k in train_examples if len(k) <= MAX_LENGTH]\n",
    "train_examples = [pad_with_zero(k) for k in train_examples]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_examples)\n",
    "\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码\n",
    "def create_mask(targets):\n",
    "\n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    \n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "\n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combine_mask\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2 import GPT2\n",
    "from decoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = num_layers * d_model\n",
    "num_heads = 8\n",
    "target_vocab_size = tokenizer_title.vocab_size + 2\n",
    "max_seq_len = MAX_LENGTH\n",
    "dropout_rate = 0.1\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# 定义优化器\n",
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, \n",
    "                                     beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 定义目标函数和评估指标\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "# 定义模型\n",
    "gpt2 = GPT2(num_layers, d_model, num_heads, dff,\n",
    "            target_vocab_size,\n",
    "            max_seq_len, \n",
    "            dropout_rate)\n",
    "\n",
    "checkpoint_path = './checkpoint/train_gpt2_exp1'\n",
    "ckpt = tf.train.Checkpoint(gpt2=gpt2,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 0, loss:1.6026, acc:0.0177\n",
      "epoch 1, batch 100, loss:1.4949, acc:0.0166\n",
      "epoch 1, batch 200, loss:1.4649, acc:0.0167\n",
      "epoch 1, batch 300, loss:1.4361, acc:0.0174\n",
      "epoch 1, batch 400, loss:1.4098, acc:0.0189\n",
      "epoch 1, batch 500, loss:1.3874, acc:0.0201\n",
      "epoch 1, batch 600, loss:1.3668, acc:0.0214\n",
      "epoch 1, batch 700, loss:1.3520, acc:0.0224\n",
      "epoch 1, batch 800, loss:1.3386, acc:0.0234\n",
      "epoch 1, batch 900, loss:1.3265, acc:0.0243\n",
      "epoch 1, batch 1000, loss:1.3154, acc:0.0251\n",
      "epoch 1, batch 1100, loss:1.3053, acc:0.0258\n",
      "epoch 1, batch 1200, loss:1.2954, acc:0.0266\n",
      "epoch 1, batch 1300, loss:1.2864, acc:0.0273\n",
      "epoch 1, batch 1400, loss:1.2778, acc:0.0280\n",
      "epoch 1, batch 1500, loss:1.2691, acc:0.0286\n",
      "epoch 1, batch 1600, loss:1.2615, acc:0.0293\n",
      "epoch 1, batch 1700, loss:1.2542, acc:0.0299\n",
      "epoch 1, batch 1800, loss:1.2477, acc:0.0305\n",
      "epoch 1, batch 1900, loss:1.2404, acc:0.0312\n",
      "epoch 1, batch 2000, loss:1.2335, acc:0.0317\n",
      "epoch 1, batch 2100, loss:1.2267, acc:0.0323\n",
      "epoch 1, batch 2200, loss:1.2205, acc:0.0328\n",
      "epoch 1, batch 2300, loss:1.2144, acc:0.0333\n",
      "epoch 1, batch 2400, loss:1.2086, acc:0.0338\n",
      "epoch 1, batch 2500, loss:1.2027, acc:0.0342\n",
      "epoch 1, batch 2600, loss:1.1974, acc:0.0347\n",
      "epoch 1, batch 2700, loss:1.1924, acc:0.0351\n",
      "epoch 1, batch 2800, loss:1.1871, acc:0.0355\n",
      "epoch 1, batch 2900, loss:1.1823, acc:0.0359\n",
      "epoch 1, batch 3000, loss:1.1778, acc:0.0363\n",
      "epoch 1, batch 3100, loss:1.1732, acc:0.0366\n",
      "epoch 1, batch 3200, loss:1.1687, acc:0.0370\n",
      "epoch 1, batch 3300, loss:1.1647, acc:0.0374\n",
      "epoch 1, batch 3400, loss:1.1608, acc:0.0377\n",
      "epoch 1, batch 3500, loss:1.1568, acc:0.0380\n",
      "epoch 1, batch 3600, loss:1.1529, acc:0.0383\n",
      "epoch 1, batch 3700, loss:1.1495, acc:0.0386\n",
      "epoch 1, batch 3800, loss:1.1461, acc:0.0389\n",
      "epoch 1, batch 3900, loss:1.1428, acc:0.0392\n",
      "epoch 1, batch 4000, loss:1.1394, acc:0.0394\n",
      "epoch 1, batch 4100, loss:1.1360, acc:0.0397\n",
      "epoch 1, batch 4200, loss:1.1329, acc:0.0399\n",
      "epoch 1, batch 4300, loss:1.1298, acc:0.0402\n",
      "epoch 1, batch 4400, loss:1.1266, acc:0.0404\n",
      "epoch 1, batch 4500, loss:1.1236, acc:0.0407\n",
      "epoch 1, batch 4600, loss:1.1205, acc:0.0409\n",
      "epoch 1, loss:1.1181, acc:0.0411\n",
      "time in 1 epoch:557.7137112617493 secs\n",
      "\n",
      "epoch 2, batch 0, loss:1.0395, acc:0.0527\n",
      "epoch 2, batch 100, loss:0.9866, acc:0.0512\n",
      "epoch 2, batch 200, loss:0.9878, acc:0.0513\n",
      "epoch 2, batch 300, loss:0.9829, acc:0.0517\n",
      "epoch 2, batch 400, loss:0.9801, acc:0.0518\n",
      "epoch 2, batch 500, loss:0.9782, acc:0.0519\n",
      "epoch 2, batch 600, loss:0.9738, acc:0.0521\n",
      "epoch 2, batch 700, loss:0.9721, acc:0.0523\n",
      "epoch 2, batch 800, loss:0.9711, acc:0.0525\n",
      "epoch 2, batch 900, loss:0.9701, acc:0.0525\n",
      "epoch 2, batch 1000, loss:0.9687, acc:0.0526\n",
      "epoch 2, batch 1100, loss:0.9674, acc:0.0528\n",
      "epoch 2, batch 1200, loss:0.9657, acc:0.0529\n",
      "epoch 2, batch 1300, loss:0.9645, acc:0.0530\n",
      "epoch 2, batch 1400, loss:0.9635, acc:0.0531\n",
      "epoch 2, batch 1500, loss:0.9620, acc:0.0532\n",
      "epoch 2, batch 1600, loss:0.9612, acc:0.0532\n",
      "epoch 2, batch 1700, loss:0.9605, acc:0.0533\n",
      "epoch 2, batch 1800, loss:0.9602, acc:0.0533\n",
      "epoch 2, batch 1900, loss:0.9587, acc:0.0535\n",
      "epoch 2, batch 2000, loss:0.9574, acc:0.0536\n",
      "epoch 2, batch 2100, loss:0.9562, acc:0.0536\n",
      "epoch 2, batch 2200, loss:0.9551, acc:0.0537\n",
      "epoch 2, batch 2300, loss:0.9540, acc:0.0538\n",
      "epoch 2, batch 2400, loss:0.9530, acc:0.0538\n",
      "epoch 2, batch 2500, loss:0.9516, acc:0.0539\n",
      "epoch 2, batch 2600, loss:0.9507, acc:0.0539\n",
      "epoch 2, batch 2700, loss:0.9498, acc:0.0540\n",
      "epoch 2, batch 2800, loss:0.9486, acc:0.0541\n",
      "epoch 2, batch 2900, loss:0.9476, acc:0.0541\n",
      "epoch 2, batch 3000, loss:0.9467, acc:0.0542\n",
      "epoch 2, batch 3100, loss:0.9456, acc:0.0543\n",
      "epoch 2, batch 3200, loss:0.9445, acc:0.0544\n",
      "epoch 2, batch 3300, loss:0.9437, acc:0.0545\n",
      "epoch 2, batch 3400, loss:0.9429, acc:0.0545\n",
      "epoch 2, batch 3500, loss:0.9419, acc:0.0546\n",
      "epoch 2, batch 3600, loss:0.9409, acc:0.0547\n",
      "epoch 2, batch 3700, loss:0.9402, acc:0.0547\n",
      "epoch 2, batch 3800, loss:0.9394, acc:0.0548\n",
      "epoch 2, batch 3900, loss:0.9386, acc:0.0549\n",
      "epoch 2, batch 4000, loss:0.9378, acc:0.0549\n",
      "epoch 2, batch 4100, loss:0.9369, acc:0.0550\n",
      "epoch 2, batch 4200, loss:0.9361, acc:0.0550\n",
      "epoch 2, batch 4300, loss:0.9353, acc:0.0551\n",
      "epoch 2, batch 4400, loss:0.9345, acc:0.0552\n",
      "epoch 2, batch 4500, loss:0.9338, acc:0.0552\n",
      "epoch 2, batch 4600, loss:0.9329, acc:0.0553\n",
      "epoch 2, save model at ./checkpoint/train_gpt2_exp1/ckpt-1\n",
      "epoch 2, loss:0.9324, acc:0.0553\n",
      "time in 1 epoch:552.0405840873718 secs\n",
      "\n",
      "epoch 3, batch 0, loss:0.9516, acc:0.0595\n",
      "epoch 3, batch 100, loss:0.9034, acc:0.0573\n",
      "epoch 3, batch 200, loss:0.9054, acc:0.0574\n",
      "epoch 3, batch 300, loss:0.9014, acc:0.0576\n",
      "epoch 3, batch 400, loss:0.8997, acc:0.0577\n",
      "epoch 3, batch 500, loss:0.8990, acc:0.0577\n",
      "epoch 3, batch 600, loss:0.8958, acc:0.0579\n",
      "epoch 3, batch 700, loss:0.8952, acc:0.0580\n",
      "epoch 3, batch 800, loss:0.8950, acc:0.0580\n",
      "epoch 3, batch 900, loss:0.8951, acc:0.0580\n",
      "epoch 3, batch 1000, loss:0.8948, acc:0.0580\n",
      "epoch 3, batch 1100, loss:0.8942, acc:0.0581\n",
      "epoch 3, batch 1200, loss:0.8934, acc:0.0581\n",
      "epoch 3, batch 1300, loss:0.8930, acc:0.0582\n",
      "epoch 3, batch 1400, loss:0.8928, acc:0.0582\n",
      "epoch 3, batch 1500, loss:0.8922, acc:0.0582\n",
      "epoch 3, batch 1600, loss:0.8922, acc:0.0582\n",
      "epoch 3, batch 1700, loss:0.8922, acc:0.0582\n",
      "epoch 3, batch 1800, loss:0.8927, acc:0.0582\n",
      "epoch 3, batch 1900, loss:0.8920, acc:0.0583\n",
      "epoch 3, batch 2000, loss:0.8914, acc:0.0584\n",
      "epoch 3, batch 2100, loss:0.8909, acc:0.0584\n",
      "epoch 3, batch 2200, loss:0.8905, acc:0.0584\n",
      "epoch 3, batch 2300, loss:0.8901, acc:0.0584\n",
      "epoch 3, batch 2400, loss:0.8898, acc:0.0585\n",
      "epoch 3, batch 2500, loss:0.8891, acc:0.0585\n",
      "epoch 3, batch 2600, loss:0.8888, acc:0.0585\n",
      "epoch 3, batch 2700, loss:0.8887, acc:0.0585\n",
      "epoch 3, batch 2800, loss:0.8880, acc:0.0585\n",
      "epoch 3, batch 2900, loss:0.8877, acc:0.0585\n",
      "epoch 3, batch 3000, loss:0.8874, acc:0.0586\n",
      "epoch 3, batch 3100, loss:0.8870, acc:0.0586\n",
      "epoch 3, batch 3200, loss:0.8864, acc:0.0587\n",
      "epoch 3, batch 3300, loss:0.8863, acc:0.0587\n",
      "epoch 3, batch 3400, loss:0.8860, acc:0.0587\n",
      "epoch 3, batch 3500, loss:0.8855, acc:0.0588\n",
      "epoch 3, batch 3600, loss:0.8850, acc:0.0588\n",
      "epoch 3, batch 3700, loss:0.8850, acc:0.0588\n",
      "epoch 3, batch 3800, loss:0.8847, acc:0.0589\n",
      "epoch 3, batch 3900, loss:0.8844, acc:0.0589\n",
      "epoch 3, batch 4000, loss:0.8842, acc:0.0589\n",
      "epoch 3, batch 4100, loss:0.8838, acc:0.0589\n",
      "epoch 3, batch 4200, loss:0.8835, acc:0.0590\n",
      "epoch 3, batch 4300, loss:0.8832, acc:0.0590\n",
      "epoch 3, batch 4400, loss:0.8829, acc:0.0590\n",
      "epoch 3, batch 4500, loss:0.8826, acc:0.0590\n",
      "epoch 3, batch 4600, loss:0.8822, acc:0.0590\n",
      "epoch 3, loss:0.8821, acc:0.0591\n",
      "time in 1 epoch:555.0140678882599 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_step(targets):\n",
    "    tar_inp = targets[:, :-1]\n",
    "    tar_real = targets[:, 1:]\n",
    "    \n",
    "    # 构造掩码\n",
    "    combined_mask = create_mask(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = gpt2(tar_inp, True, combined_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "        \n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, gpt2.trainable_variables)\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, gpt2.trainable_variables))\n",
    "\n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "EPOCHS = 3\n",
    "step_list = []\n",
    "loss_list = []\n",
    "step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for batch, all_inputs in enumerate(train_dataset):\n",
    "        \n",
    "        # 训练\n",
    "        train_step(all_inputs)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = train_loss.result()\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "                epoch+1, batch, loss, train_accuracy.result()\n",
    "            ))\n",
    "            step_list.append(step)\n",
    "            loss_list.append(loss)\n",
    "        step += 1\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch {}, save model at {}'.format(\n",
    "        epoch+1, ckpt_save_path\n",
    "        ))\n",
    "\n",
    "\n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "        epoch+1, train_loss.result(), train_accuracy.result()\n",
    "    ))\n",
    "\n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_title.vocab_size]\n",
    "    end_token = [tokenizer_title.vocab_size + 1]\n",
    "    \n",
    "    # 增加开始和结束标记\n",
    "    inp_sentence = start_token + tokenizer_title.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    decoder_input = [tokenizer_title.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        \n",
    "        combined_mask = create_mask(encoder_input)\n",
    "        \n",
    "        predictions, _ = gpt2(encoder_input, False, combined_mask)\n",
    "\n",
    "        # 从 seq_len 维度选择最后一个词\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 如果 predicted_id 等于结束标记，就返回结果\n",
    "        if predicted_id == tokenizer_title.vocab_size + 1:\n",
    "            return tf.squeeze(encoder_input, axis=0)\n",
    "\n",
    "        # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n",
    "        encoder_input = tf.concat([encoder_input, predicted_id], axis=-1)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    \n",
    "    return tf.squeeze(encoder_input, axis=0)\n",
    "    \n",
    "def translate(sentence, plot=''):\n",
    "    result = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_title.decode([i for i in result if i < tokenizer_title.vocab_size]) \n",
    "    predicted_sentence = predicted_sentence.replace(\" \", \"\")\n",
    "    sentence = sentence.replace(\" \", \"\")\n",
    "\n",
    "    print('输入: {}'.format(sentence))\n",
    "    print('预测输出: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实数据： 当你的车离合线断了，汽修厂让你自己开过去，你该怎么做才能把车送到汽修厂？|汽车\n",
      "输入: 当你的车离合线断了，汽修厂让你自己开过去，你该怎么做才能把车送到汽修厂？\n",
      "预测输出: 当你的车离合线断了，汽修厂让你自己开过去，你该怎么做才能把车送到汽修厂？的问题？|汽车\n",
      "============================\n",
      "真实数据： 葡媒：国米、拉齐奥关注波尔图主帅孔塞桑|体育\n",
      "输入: 葡媒：国米、拉齐奥关注波尔图主帅孔塞桑\n",
      "预测输出: 葡媒：国米、拉齐奥关注波尔图主帅孔塞桑训练|体育\n",
      "============================\n",
      "真实数据： 今日将爆发最强内战，天津权健vs广州恒大，阿兰能否带领恒大取胜|体育\n",
      "输入: 今日将爆发最强内战，天津权健vs广州恒大，阿兰能否带领恒大取胜\n",
      "预测输出: 今日将爆发最强内战，天津权健vs广州恒大，阿兰能否带领恒大取胜进球？|体育\n",
      "============================\n",
      "真实数据： 为什么说历史不忍细读呢？|文化\n",
      "输入: 为什么说历史不忍细读呢？\n",
      "预测输出: 为什么说历史不忍细读呢？的是什么原因？|文化\n"
     ]
    }
   ],
   "source": [
    "test = corpus_format[300000:]\n",
    "i = 0\n",
    "print(\"真实数据：\", test[i].replace(\" \", \"\"))\n",
    "translate(test[i][:-4])\n",
    "print(\"============================\")\n",
    "i = 1\n",
    "print(\"真实数据：\", test[i].replace(\" \", \"\"))\n",
    "translate(test[i][:-4])\n",
    "print(\"============================\")\n",
    "i = 2\n",
    "print(\"真实数据：\", test[i].replace(\" \", \"\"))\n",
    "translate(test[i][:-4])\n",
    "print(\"============================\")\n",
    "i = 3\n",
    "print(\"真实数据：\", test[i].replace(\" \", \"\"))\n",
    "translate(test[i][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: 小麦收成\n",
      "预测输出: 小麦收成，玉米的价格会怎么样？|农业\n",
      "============================\n",
      "输入: 整容狂人\n",
      "预测输出: 整容狂人的女星，网友：这是你的最爱的吗？|娱乐\n",
      "============================\n",
      "输入: 北大校长口误\n",
      "预测输出: 北大校长口误的孩子们都在干什么？|教育\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "s = \" \".join(list(jieba.cut(\"小麦收成\")))\n",
    "translate(s)\n",
    "print(\"============================\")\n",
    "s = \" \".join(list(jieba.cut(\"整容狂人\")))\n",
    "translate(s)\n",
    "print(\"============================\")\n",
    "s = \" \".join(list(jieba.cut(\"北大校长口误\")))\n",
    "translate(s)\n",
    "print(\"============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
