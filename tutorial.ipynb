{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80d2f6b-c417-483f-a840-858881d4f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 08:05:13.977966: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from gpt1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bcbdd2-74ea-46bd-863d-abb25359fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LENGTH = 40\n",
    "cat_name_dic = {\n",
    "    '100': '民生',\n",
    "    '101': '文化',\n",
    "    '102': '娱乐',\n",
    "    '103': '体育',\n",
    "    '104': '财经',\n",
    "    '106': '房产',\n",
    "    '107': '汽车',\n",
    "    '108': '教育',\n",
    "    '109': '科技',\n",
    "    '110': '军事',\n",
    "    '112': '旅游',\n",
    "    '113': '国际',\n",
    "    '114': '证券',\n",
    "    '115': '农业',\n",
    "    '116': '电竞'\n",
    "}\n",
    "cat_name_all = list(cat_name_dic.values())\n",
    "cat_name_label = dict([(cat_name_all[k], k) for k in range(len(cat_name_all))])\n",
    "\n",
    "def load_data(file_path):\n",
    "    corpus = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for k in f:\n",
    "            new_id, cat, cat_n, title, title_kws = k.strip(\"\").split(\"_!_\")\n",
    "            cat_name = cat_name_dic.get(cat, '')\n",
    "            if cat_name == '':\n",
    "                continue\n",
    "            if len(title) > MAX_LENGTH:\n",
    "                continue\n",
    "                \n",
    "            label = [0 for i in range(len(cat_name_all))]\n",
    "            index = cat_name_label[cat_name]\n",
    "            label[index] = 1\n",
    "            corpus.append([title, label])\n",
    "    return corpus\n",
    "\n",
    "corpus = load_data('./toutiao_cat_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32422c8-a40c-4c98-8576-2e8800db14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940ac624-d75d-45fc-972f-d5789db66d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.630 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "### 分词\n",
    "corpus_format = []\n",
    "for k in corpus:\n",
    "    title = k[0]\n",
    "    cat = k[1]\n",
    "    title = \" \".join(jieba.cut(title, cut_all=False))\n",
    "    corpus_format.append([title, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fad3168-dcc2-4db6-9552-37bd7b21219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(corpus_format)\n",
    "train_examples, val_examples = corpus_format[:300000], corpus_format[300000:]\n",
    "tokenizer_title = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (k[0] for k in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389703c5-588c-4245-b581-aedae09ea751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8152"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_title.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f2717c-8d26-4c44-8633-a7c70ed54bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang):\n",
    "    lang1, lang2 = lang\n",
    "    lang1 = [tokenizer_title.vocab_size] + tokenizer_title.encode(lang1) + [tokenizer_title.vocab_size + 1]\n",
    "    return [lang1, lang2]\n",
    "\n",
    "def filter_long_sent(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)\n",
    "\n",
    "def pad_with_zero(lang, max_length=MAX_LENGTH):\n",
    "    lang1, lang2 = lang\n",
    "    n1 = MAX_LENGTH - len(lang1)\n",
    "    lang1 = lang1 + [0 for k in range(n1)]\n",
    "    return [lang1, lang2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bf40b7-5dab-4758-9011-853764c0e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [encode(k) for k in train_examples]\n",
    "train_examples = [k for k in train_examples if len(k[0]) <= MAX_LENGTH]\n",
    "train_examples = [pad_with_zero(k) for k in train_examples]\n",
    "dic = {}\n",
    "dic['title'] = [k[0] for k in train_examples]\n",
    "dic['cat'] = [k[1] for k in train_examples]\n",
    "train_examples = dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05dd6cc9-ace1-4e16-a0e4-9baefbfd8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 08:08:28.614737: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-14 08:08:28.652895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:40:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2023-06-14 08:08:28.652923: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-14 08:08:28.657223: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-14 08:08:28.657286: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-14 08:08:28.658219: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-14 08:08:28.658884: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-14 08:08:28.659586: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-06-14 08:08:28.660290: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-06-14 08:08:28.660450: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-06-14 08:08:28.661142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-14 08:08:28.661551: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 08:08:28.665433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:40:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2023-06-14 08:08:28.666074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-06-14 08:08:28.666105: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-06-14 08:08:29.187571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-14 08:08:29.187590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-06-14 08:08:29.187595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-06-14 08:08:29.188711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8098 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:40:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922fa4e5-7e91-4fc6-9bdd-c8d252362648",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4845c7c6-ea97-4a1b-b562-87e961561e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_examples = [encode(k) for k in val_examples]\n",
    "val_examples = [k for k in val_examples if len(k[0]) <= MAX_LENGTH]\n",
    "val_examples = [pad_with_zero(k) for k in val_examples]\n",
    "dic['title'] = [k[0] for k in val_examples]\n",
    "dic['cat'] = [k[1] for k in val_examples]\n",
    "val_examples = dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603f09d3-bf53-462c-a62c-48397f22dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b8cec6-86c6-474a-a8ff-319ba91010bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "target_vocab_size = tokenizer_title.vocab_size + 2\n",
    "max_seq_len = MAX_LENGTH\n",
    "dropout_rate = 0.1\n",
    "n_class = len(cat_name_dic)\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965b9ad9-d9f6-4e1c-aa6d-86d1ee4a8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# 定义优化器\n",
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, \n",
    "                                     beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ef7e43-f18d-42d3-a360-3e1b5a30a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60047352-5b33-4603-b422-ea52287221ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, diff,\n",
    "                 target_vocab_size, \n",
    "                 max_seq_len, \n",
    "                 fine_tuning_class_num, \n",
    "                 drop_rate=0.1):\n",
    "        super(GPT1, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, diff,\n",
    "                              target_vocab_size, max_seq_len, drop_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        self.fine_tuning_layer = tf.keras.layers.Dense(fine_tuning_class_num)\n",
    "        \n",
    "    def call(self, targets, training, look_ahead_mask):\n",
    "\n",
    "        decode_out, att_weights = self.decoder(targets, training, \n",
    "                                               look_ahead_mask)\n",
    "        final_out = self.final_layer(decode_out)\n",
    "\n",
    "        return final_out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1696abe5-6950-4faf-9caa-596e22aa8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "gpt1 = GPT1(num_layers, d_model, num_heads, dff,\n",
    "            target_vocab_size,\n",
    "            max_seq_len, \n",
    "            n_class,\n",
    "            dropout_rate)\n",
    "\n",
    "checkpoint_path = './checkpoint/train_cat'\n",
    "ckpt = tf.train.Checkpoint(gpt1=gpt1,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5190e3-781a-4210-acfb-622b6c48fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码\n",
    "def create_mask(targets):\n",
    "\n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    \n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "\n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combine_mask\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def train_step(targets):\n",
    "    tar_inp = targets['title'][:, :-1]\n",
    "    tar_real = targets['title'][:, 1:]\n",
    "    cat_name = targets['cat']\n",
    "    \n",
    "    # 构造掩码\n",
    "    combined_mask = create_mask(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = gpt1(tar_inp, True, combined_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "        \n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, gpt1.trainable_variables)\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, gpt1.trainable_variables))\n",
    "\n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32afedfe-2516-420e-9991-6e5e15375951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 08:09:11.082684: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-06-14 08:09:11.784172: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-06-14 08:09:11.784219: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 0, loss:4.1657\n",
      "epoch 1, batch 1000, loss:3.7795\n",
      "epoch 1, batch 2000, loss:3.4486\n",
      "epoch 1, batch 3000, loss:3.2612\n",
      "epoch 1, batch 4000, loss:3.1223\n",
      "epoch 1, loss:3.0475, acc:0.0825\n",
      "time in 1 epoch:436.98212456703186 secs\n",
      "\n",
      "epoch 2, batch 0, loss:2.3715\n",
      "epoch 2, batch 1000, loss:2.5270\n",
      "epoch 2, batch 2000, loss:2.4902\n",
      "epoch 2, batch 3000, loss:2.4668\n",
      "epoch 2, batch 4000, loss:2.4436\n",
      "epoch 2, save model at ./checkpoint/train_cat/ckpt-1\n",
      "epoch 2, loss:2.4312, acc:0.1209\n",
      "time in 1 epoch:436.51859426498413 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "step_list = []\n",
    "loss_list = []\n",
    "step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for batch, all_inputs in enumerate(train_dataset):\n",
    "        \n",
    "        # 训练\n",
    "        train_step(all_inputs)\n",
    "        \n",
    "        # gpt1.summary()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss = train_loss.result()\n",
    "            print('epoch {}, batch {}, loss:{:.4f}'.format(\n",
    "                epoch+1, batch, loss\n",
    "            )) \n",
    "            step_list.append(step)\n",
    "            loss_list.append(loss)\n",
    "        step += 1\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch {}, save model at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(epoch+1, train_loss.result(), train_accuracy.result()))\n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6bf1187-43a7-4dab-a971-7ea2eab0b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fine tune 任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23221f21-d487-400d-b3ad-f484b4c2199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1FT(tf.keras.Model):\n",
    "    def __init__(self, gpt):\n",
    "        super(GPT1FT, self).__init__()\n",
    "        self.gpt = gpt\n",
    "        \n",
    "    def call(self, targets, training, look_ahead_mask):\n",
    "        fo, _ = self.gpt(targets, training, look_ahead_mask)\n",
    "        fine_tuning_out = self.gpt.fine_tuning_layer(tf.keras.layers.Flatten()(fo))\n",
    "        \n",
    "        return fine_tuning_out\n",
    "\n",
    "gpt1_ft = GPT1FT(gpt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bc17795-591b-4b2a-9679-583e6436fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last checkpoit restore\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './checkpoint/train_cat'\n",
    "ckpt = tf.train.Checkpoint(gpt1=gpt1,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a95bca-f23b-4b11-b02d-5275eaeef8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt1_ft.gpt.decoder.trainable = False\n",
    "gpt1_ft.gpt.final_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e37b2e6-51da-41b8-902c-879bfa440b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_ft = './checkpoint/train_cat_ft'\n",
    "ckpt_ft = tf.train.Checkpoint(gpt1_ft=gpt1_ft,\n",
    "                              optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager_ft = tf.train.CheckpointManager(ckpt_ft, checkpoint_path_ft, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager_ft.latest_checkpoint:\n",
    "    ckpt_ft.restore(ckpt_manager_ft.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d32fbf-e8b5-460d-afb7-0ad2ba07fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object_fine_tuning = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss_fine_tuning = tf.keras.metrics.Mean(name='train_loss_fine_tuning')\n",
    "train_accuracy_fine_tuning = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy_fine_tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f943ccd3-0c15-4d2c-b88c-a6503ff157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun_fine_tuning(y_ture, y_pred):\n",
    "    loss_ = loss_object_fine_tuning(y_ture, y_pred)\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def train_step(targets):\n",
    "    tar_inp = targets['title'][:, :-1]\n",
    "    tar_real = targets['title'][:, 1:]\n",
    "    cat_name = targets['cat']\n",
    "    \n",
    "    # 构造掩码\n",
    "    combined_mask = create_mask(tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predict_fine_tuning = gpt1_ft(tar_inp, True, combined_mask)\n",
    "        loss_fine_tuning = loss_fun_fine_tuning(cat_name, predict_fine_tuning)\n",
    "        \n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss_fine_tuning, gpt1_ft.trainable_variables)\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, gpt1_ft.trainable_variables))\n",
    "\n",
    "    # 记录loss和准确率\n",
    "    train_loss_fine_tuning(loss_fine_tuning)\n",
    "    train_accuracy_fine_tuning(cat_name, predict_fine_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98017b32-ed6c-4150-883d-145996c2f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 0, loss:2.4312, loss_fine:137.3423, acc:0.6250\n",
      "epoch 1, batch 1000, loss:2.4312, loss_fine:135.3369, acc:0.6243\n",
      "epoch 1, batch 2000, loss:2.4312, loss_fine:132.9328, acc:0.6261\n",
      "epoch 1, batch 3000, loss:2.4312, loss_fine:131.2797, acc:0.6275\n",
      "epoch 1, batch 4000, loss:2.4312, loss_fine:129.4021, acc:0.6294\n",
      "epoch 1, loss:2.4312, acc:0.1209\n",
      "time in 1 epoch:350.19091749191284 secs\n",
      "\n",
      "epoch 2, batch 0, loss:2.4312, loss_fine:158.8293, acc:0.6406\n",
      "epoch 2, batch 1000, loss:2.4312, loss_fine:122.4613, acc:0.6370\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "step_list = []\n",
    "loss_list = []\n",
    "loss_list_fine_tuning = []\n",
    "step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # 重置记录项\n",
    "    train_loss_fine_tuning.reset_states()\n",
    "    train_accuracy_fine_tuning.reset_states()\n",
    "\n",
    "    for batch, all_inputs in enumerate(train_dataset):\n",
    "        \n",
    "        # 训练\n",
    "        train_step(all_inputs)\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            loss = train_loss.result()\n",
    "            loss_fine_tuning = train_loss_fine_tuning.result()\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, loss_fine:{:.4f}, acc:{:.4f}'.format(\n",
    "                epoch+1, batch, loss, loss_fine_tuning, train_accuracy_fine_tuning.result()\n",
    "            )) \n",
    "            step_list.append(step)\n",
    "            loss_list.append(loss)\n",
    "            loss_list_fine_tuning.append(loss_fine_tuning)\n",
    "        step += 1\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager_ft.save()\n",
    "        print('epoch {}, save model at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(epoch+1, train_loss.result(), train_accuracy.result()))\n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7752e312-f2c0-4c1f-b0b5-08a65c639b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    \n",
    "    start_token = [tokenizer_title.vocab_size]\n",
    "    end_token = [tokenizer_title.vocab_size + 1]\n",
    "    inp_sentence = start_token + tokenizer_title.encode(inp_sentence) + end_token\n",
    "    n = MAX_LENGTH - len(inp_sentence)\n",
    "    inp_sentence = inp_sentence + [0 for k in range(n)]\n",
    "    inp_sentence = inp_sentence[:-1]\n",
    "    inp_sentence = tf.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    combined_mask = create_mask(inp_sentence)\n",
    "    predict_fine_tuning = gpt1_ft(inp_sentence, False, combined_mask)\n",
    "    predicted_id = tf.cast(tf.argmax(predict_fine_tuning, axis=-1), tf.int32)\n",
    "    return predicted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03f85b71-1cc8-4c45-84d0-86bb6a0fbcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_name(sentence, plot=''):\n",
    "    result = evaluate(sentence)[0]\n",
    "    result = cat_name_all[result]\n",
    "\n",
    "    print('输入: {}'.format(sentence).replace(\" \", \"\"))\n",
    "    print('预测输出: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92cc4a5c-d980-4a8b-82d5-39d654cd5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:《狂飙》结局后，张译终于发声了，剧中演员回应一辈子不想见张译\n",
      "预测输出: 娱乐\n"
     ]
    }
   ],
   "source": [
    "s = \"《狂飙》结局后，张译终于发声了，剧中演员回应一辈子不想见张译\"\n",
    "s = \" \".join(jieba.cut(s))\n",
    "get_cat_name(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e8e410-585f-465c-891e-c3206cf479f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:教育部下发新通知，将调整今年的高考方向，家长看完心态“崩”了\n",
      "预测输出: 教育\n"
     ]
    }
   ],
   "source": [
    "s = \"教育部下发新通知，将调整今年的高考方向，家长看完心态“崩”了\"\n",
    "s = \" \".join(jieba.cut(s))\n",
    "get_cat_name(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "688e7ac7-30aa-465e-8452-b545a8c5ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:俄罗斯学会了，发射大批气球飞向乌克兰，乌军导弹快不够用了\n",
      "预测输出: 军事\n"
     ]
    }
   ],
   "source": [
    "s = \"俄罗斯学会了，发射大批气球飞向乌克兰，乌军导弹快不够用了\"\n",
    "s = \" \".join(jieba.cut(s))\n",
    "get_cat_name(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f000d0-b8ee-4c8d-82b2-1b59b90c2237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入:今年小麦收成不如往年好\n",
      "预测输出: 农业\n"
     ]
    }
   ],
   "source": [
    "s = \"今年小麦收成不如往年好\"\n",
    "s = \" \".join(jieba.cut(s))\n",
    "get_cat_name(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a31f6-b5a6-44a3-aceb-687733a02e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
